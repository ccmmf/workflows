---
output:
  pdf_document: default
  html_document: default
---
# Modular & Reproducible PEcAn workflows

## Table of contents
1. [Introduction](#introduction)
2. [Design Rationale](#design)
3. [Obtaining PEcAn resources](#obtainingresources)
4. [Head-node installation](#headnodeinstallation)
5. [Distributed PEcAn Workflows](#distributedpecan)
6. [Dependencies](#dependencies)

## Introduction <a name="introduction"></a>
This document is intended to help with the initial set-up and configuration needed to support execution of PEcAn workflows on a Slurm-backed HPC cluster.

This approach is intended to:

- Run PEcAn workflows at-scale via Slurm & Apptainer
- Enable transparency, re-usability, and reproducability within PEcAn workflows
- Minimize maintenance required on installed software on the CARB cluster

## Design Rationale <a name="design"></a>
The workflow framework described below is intended to provide CARB with a convenient interface to execute PEcAn-based workflows at scale, without manually managing the distribution of computational work, and maintaining transparency with regards to the entire pipeline.

### Workflow execution and data inventory
At the highest level, the framework heavily depends on [Targets](https://books.ropensci.org/targets/) ([git](https://github.com/ropensci-books/targets/)) to manage the workflow execution aspects of PEcAn analyses.  

When a PEcAn workflow is invoked, if a novel run identifier is provided, a new directory is created for the execution. The workflow script (_targets.R) is then written to this new directory, and the run-time parameters (denoted by '@' symbols in the main script) are written to the new script file. 

It is critical to understand that when a workflow is executed, the working directory of the R processes associated with that workflow will be the individual workflow run directory - _not_ the directory from which the workflow is invoked. It is also not possible to change the working directory of the R process during the execution of a workflow.

In addition to the workflow script, the workflow run directory will contain all artifacts which are created as part of the run. This means that a CARB scientist can run successive, iterative versions of each workflow until the desired outcome is acheived. Each individual run is preserved in its entirety, and the scientist can always reference the specific workflow run which produced the desired outcome by its unique run identifier.

By referencing the specific workflow run (by its identifier), an individual is also able to reference the specific data artifacts generated by that workflow.

### Workflow re-evaluation
One of the benefits of using a workflow framework is that we are enabled to leverage efficient workflow run re-evaluation. 

This means that if a workflow is invoked with a run identifier that already exists, that invocation will only execute steps of the workflow if either the inputs to that step have changed, or if the code for that workflow step has changed.

### Data Referencing
As part of the workflow framework established, an individual is able to reference data external to a workflow by invoking specific workflow steps within the workflow definition. Specifically, the data artifacts of a particular run of Workflow A may be referenced by Workflow B, using the run identifier of the specific iteration of Workflow A desired. This allows the creation of modular, extensible workflows which depend on common data resources from earlier steps.

Disciplined execution of workflows with attention paid to run identifiers will enable the creation of standardized validated data products suitable for use by a broad array of data scientists.

See the **(HP: need to create a multipart workflow example)** for an example of creating a data handling workflow prior to an analytics workflow.

### Distributed Compute for Workflows
In order to execute the workflow in a distributed manner, individual workflow steps are invoked within the specific workflow run directory. To accomplish this, a new R process is instantiated on the worker node, and the code is executed as part of the Targets framework.

This means that the workflow steps - as invoked by slurm within an R process - have access to the workflow run resources, such as data artifacts produced by preceding steps. It will also be executed in the context of the workflow run directory, and so the invocation of PEcAn methods within the workflow directory becomes quite direct. This should also make it clear that, as the step is invoked within a new R namespace (and indeed, on an entirely different compute node), each workflow step must import its own dependencies.

### Custom workflow steps
Custom workflow steps can be created by any user. They must only be sourced into the workflow scripts (see: workflow method sourcing).

Workflow steps are executed as part of a targets-mediated workflow run. The code contained within a workflow step is invoked from within a workflow run directory. Depending on the method of execution, the code may be executed within a namespace local to the node which invoked the workflow, or it may be executed within a container on a slurm-managed compute node.

Therefore, it is advised that each custom workflow step should explicitly import its dependencies, as it cannot be assumed that the executing namespace will contain these dependencies by default.

## Obtaining PEcAn Resources {#obtainingresources}

An advantage to using a workflow framework for PEcAn workflow execution can be seen by observing a simple data logistics workflow. 

**note: the workflow identified in this readme expects that the various AWS resources are already installed and configured by the user. Please see the #dependencies section.**

A simple workflow which obtains the needed data products from the CCMMF AWS respository and unpacks them can be seen below (excerpt from the **link to data prep workflow** file):

```R
  list(
    # source data handling
    tar_target(ccmmf_data_tarball, download_ccmmf_data(prefix_url=ccmmf_data_tarball_url, local_path=tar_path_store(), prefix_filename=ccmmf_data_filename)),
    tar_target(obtained_resources_untar, untar(ccmmf_data_tarball, list = TRUE)) 
  )
```

This simple workflow will execute in the current working directory of the current R process, and:
1. access the CCMMF S3 data store using existing credentials
2. download the tarball specified as part of the workflow
3. register the tarball as a data artifact
4. decompress the tarball and list its contents

If this workflow is invoked again under the same run identifier, after step 2 is complete, the workflow will evaluate the tarball downloaded during the 2nd invocation, and compare it to the tarball obtained during the first run. If the tarball is the same, step 4 will not be executed.




Load the needed software modules:
```sh
module load apptainer
```

Apptainers will be leveraged to execute code on each of the slurm-managed nodes. This enables the user to not need to download any of the model-specific PEcAn code. It also enables the execution of different versions of PEcAn models without the need to reinstall the PEcAn stack. By simply identifying and leveraging a different version of the PEcAn model docker container, an analysis can be run with a different version of the code.

Obtain the needed dockers for this workflow, via:
```sh
apptainer pull docker://pecan/model-sipnet-git:latest
```
With data in place, the config and scripts in place, the apptainer pulled, we are now ready to run the workflow.
This has two steps. The first is a direct run of a method to generate the needed runtime configurations based on sipnet:


## Dependencies {#dependencies}
### CARB-HPC Head-node

#### Environment Modules

This guide and related files expect that the [Environment Modules](https://modules.sourceforge.net/) system is available on the CARB HPC cluster.

#### AWS S3 CLI

As written, this guide uses the AWS S3 CLI tools to move files between the remote NCSA S3 data host and the local CARB head-node. 

The environment tarball and data artifacts have been hosted by NCSA, and can be obtained via the S3 protocol from:
```sh
s3.garage.ccmmf.ncsa.cloud
```

Typically, you will be able to leverage the AWS CLI toolset to access these resources.

Once you enter the needed Access key and Secret Access Key, e.g.:
```sh
AWS Access Key ID [None]: GK8bb0d9c6b355c9a25b0b67fa
AWS Secret Access Key [None]: <-- secret key to be passed via other method -->
Default region name [None]: garage
Default output format [None]: 
```


#### Conda

This guide and the files provided with it leverage Conda for environment management. [Miniconda](https://www.anaconda.com/docs/getting-started/miniconda/main) is an excellent alternative to a full Conda installation.


The pre-packaged headnode environment can be obtained from the S3 data host with this command:
```sh
aws s3 cp --endpoint-url https://s3.garage.ccmmf.ncsa.cloud \
  s3://carb/environments/PEcAn-head.tar.gz ./

```

If you have not used conda before, it is suggested you unpack this environment into the standard location:
```sh
mkdir -p ~/.conda/envs/PEcAn-head
tar -xzf PEcAn-head.tar.gz -C ~/.conda/envs/PEcAn-head
source ~/.conda/envs/PEcAn-head/bin/activate
```
```sh
conda-unpack
```
At this point, the conda environment is unpacked, and the 'conda-unpack' command has adjusted the paths within the environment to match your local filesystem. You should be able to interrogate the conda environment's installation of R to confirm this:

```sh
Rscript -e '.libPaths()'
```
This should yield output that points to the R-library location within the unpacked conda environment.
```sh
[1] "/home/hdpriest/.conda/envs/PEcAn-head/lib/R/library"
# the above path will reflect local file system home and user specifics
```

In addition, you should be able to access the portions of the PEcAn software stack that are needed on the headnode of the cluster:
```sh
Rscript -e 'library("PEcAn.workflow")'
```
or
```sh
Rscript -e 'library("PEcAn.remote")'
```
You __will__ need to have this environment activated when executing work in a Slurm-scheduled manner, as the job submissions to the Slurm schedule are enabled via PEcAn methods.

Typically, this environment can be activated via:
```sh
conda activate PEcAn-head
```


#### Slurm

This guide and provided files have been constructed with the intention of running distributed workflows via the Slurm job scheduling system. It is assumed that the user leveraging this workflow will have a working knowledge of Slurm, but no elevated permissions will be required for interacting with Slurm resources and commands.

#### Apptainer

This guide and related files are based on the [PEcAn Docker container stacks](https://hub.docker.com/u/pecan), and are instantiated in an HPC environment via [Apptainer](https://apptainer.org/). This enables changes made to the Docker images by the PEcan community to be directly available to CARB, while also ensuring that the containers generated are compatible with the HPC environment.


