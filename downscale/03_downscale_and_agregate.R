#' ---
#' Title: Downscale and Agregate Woody Crop SOC stocks
#' author: "David LeBauer"
#' ---
#'

#'
#' # Overview
#'
#' This workflow will:
#'
#' - Use environmental covariates to predict SIPNET estimated SOC for each woody crop field in the LandIQ dataset
#'   - Uses Random Forest [maybe change to CNN later] trained on site-scale model runs.
#'   - Build a model for each ensemble member
#' - Write out a table with predicted biomass and SOC to maintain ensemble structure, ensuring correct error propagation and spatial covariance.
#' - Aggregate County-level biomass and SOC inventories
#'
## ----setup--------------------------------------------------------------------
# remotes::install_github("dlebauer/pecan@ensemble_downscaling", subdir = "modules/assim.sequential", ref = "da96331")
library(tidyverse)
library(sf)
library(terra)
devtools::load_all(here::here("../pecan/modules/assim.sequential/"))
# library(PEcAnAssimSequential)

options(readr.show_col_types = FALSE)

#' ## Get Site Level Outputs
#'
#' Next we read in site-level model outputs generated by SIPNETWOPET in 02_anchorsite_simulations.R.
#'
#' TODO:
#' - replace SIPNETWOPET outputs with SIPNET results following regional runs
#'
## -----------------------------------------------------------------------------

ensemble_data <- readRDS("cache/ensemble_data.rds")

#' ### Random Forest using PEcAn downscale workflow
#'
#'
## -----------------------------------------------------------------------------
site_coords <- readr::read_csv("data/ca_field_attributes.csv") |>
  filter(pft == "woody perennial crop") |>
  select(id, lon, lat, year)

covariates <- load("data/data_for_clust_with_ids.rda") |>
  get() # maybe there is a less convoluted way; maybe I just need to rename data_for_clust...

covariates_points <- site_coords |>
  left_join(covariates, by = "id")
covariates_sf <- covariates_points |>
  sf::st_as_sf(coords = c("lon", "lat"), crs = "EPSG:4326")

# Preprocess using renamed objects and updated ensemble data with date

####################### Start Here ############################
# Find example inputs: `find "/projectnb/dietzelab/jploshay/" -name "*.rds"`
preprocessed <- SDA_downscale_preprocess(
  ensemble_data = ensemble_data,
  site_coords   = site_coords,
  date          = "2020-01-01",
  carbon_pool   = "SOC"
)


## Next steps
debugonce(SDA_downscale)
## Stop at randomForest and see what is expected vs what is provided
## Error in model.frame.default(formula = formula, data = train_data, na.action = function (object,  :
##  invalid type (list) for variable 'ensemble1'


# Downscale the data
downscale_output <- SDA_downscale(
  preprocessed = preprocessed,
  carbon_pool  = "SOC",
  covariates   = covariates_vect,
  model_type   = "rf",
  seed         = 123
)



metrics <- SDA_downscale_metrics(downscale_output, carbon_pool = "SOC")
print(metrics)


#'
#'
#' ## Aggregate to County Level
#'
## -----------------------------------------------------------------------------
library(sf)
library(dplyr)

# Load CA county boundaries
# These are provided by Cal-Adapt as 'Areas of Interest'
county_boundaries <- st_read("data/counties.gpkg")

# check if attributes has county name
# Append county name to predicted table
grid_with_counties <- st_join(ca_grid, county_boundaries, join = st_intersects)

# Calculate county-level mean, median, and standard deviation.
county_aggregates <- grid_with_counties |>
  st_drop_geometry() |> # drop geometry for faster summarization
  group_by(county_name) |> # replace with your actual county identifier
  summarize(
    mean_biomass   = mean(predicted_biomass, na.rm = TRUE),
    median_biomass = median(predicted_biomass, na.rm = TRUE),
    sd_biomass     = sd(predicted_biomass, na.rm = TRUE)
  )

print(county_aggregates)

# For state-level, do the same but don't group_by county

#' ````
